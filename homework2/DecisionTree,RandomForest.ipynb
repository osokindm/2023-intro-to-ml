{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "928e29d0-74bf-4adc-9190-7fc175256f26",
   "metadata": {},
   "source": [
    "# Введение в машинное обучение для Java-разработчиков\n",
    "### Практическое задание 2.  Деревья решений, случайный лес.\n",
    "### Дата выдачи: 26.10.2023\n",
    "\n",
    "### Дедлайн: 23:59MSK 08.11.2023\n",
    "\n",
    "## О задании\n",
    "В этом задании мы попытаемся разобраться в устройстве деревьев решений и ансамбля на основе деревьев. \n",
    "\n",
    "## Оценивание и штрафы\n",
    "Каждая из задач (помечены тегом [task]) имеет определенное количество баллов (указана в скобках около задачи). Максимально допустимая оценка за работу — 15 баллов. \n",
    "\n",
    "- от 4 до 8 баллов - оценка \"3\"\n",
    "- от 8 до 14 баллов - оценка \"4\"\n",
    "- 15 баллов - оценка \"5\"\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов, что автоматически ведет к несдаче курса. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в комментариях. \n",
    "В данном задании есть необязательные бонусные задания, выполнение которых добавляет баллы в карму :)\n",
    "\n",
    "## Формат сдачи\n",
    "Задания сдаются путем форка основного репозитория, коммита решения в мастер-ветку вашего форка и оповещении преподавателя о выполнении ДЗ. \n",
    "\n",
    "Для удобства проверки самостоятельно посчитайте свою максимальную оценку (исходя из набора решенных задач) и укажите ниже.\n",
    "\n",
    "[[Укажите количество набранных баллов]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baea62d-018e-48c0-bc80-1de460f4472d",
   "metadata": {},
   "source": [
    "## Часть 1. Дерево решений\n",
    "\n",
    "- [task] Реализуйте подбор признака и порога для поиска условия разбиения выборки методом перебора (2 балла)\n",
    "- [task] Реализуйте метод predict для инференса дерева на датасете (1 балл)\n",
    "- [task] Обучите дерево на датасете ирисов Фишера (1 балл)\n",
    "- Бонусное задание: В методе `_calculate_leaf_value` возвращайте вероятность классов\n",
    "- Бонусное задание: Замените критерий Джини на энтропийный критерий\n",
    "4 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5cf154b-80bc-4dc4-9580-edfd099ea98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DecisionNode:\n",
    "    def __init__(self, feature=None, threshold=None, value=None, true_branch=None, false_branch=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=5):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        if depth == self.max_depth or len(y) < self.min_samples_split:\n",
    "            return DecisionNode(value=self._calculate_leaf_value(y))\n",
    "\n",
    "        best_feature, best_threshold = self._find_best_split(X, y)\n",
    "        if best_feature is None or best_threshold is None:\n",
    "            return DecisionNode(value=self._calculate_leaf_value(y))\n",
    "\n",
    "        mask = X[:, best_feature] <= best_threshold\n",
    "        true_branch = self._build_tree(X[mask], y[mask], depth + 1)\n",
    "        false_branch = self._build_tree(X[~mask], y[~mask], depth + 1)\n",
    "\n",
    "        return DecisionNode(feature=best_feature, threshold=best_threshold, true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_gain = 0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        ### \n",
    "        ### Your code here \n",
    "        ###\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _calculate_gain(self, y, y_true, y_false):\n",
    "        p = len(y_true) / len(y)\n",
    "        impurity_before = self._gini_impurity(y)\n",
    "        impurity_after = p * self._gini_impurity(y_true) + (1 - p) * self._gini_impurity(y_false)\n",
    "        return impurity_before - impurity_after\n",
    "\n",
    "    def _gini_impurity(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities ** 2)\n",
    "\n",
    "    def _calculate_leaf_value(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        most_common_class = classes[np.argmax(counts)]\n",
    "        return most_common_class\n",
    "\n",
    "    def predict(self, X):\n",
    "        ### \n",
    "        ### Your code here \n",
    "        ###\n",
    "\n",
    "def print_tree(node, depth=0):\n",
    "    indent = \"    \" * depth\n",
    "    if node.value is not None:\n",
    "        print(indent + \"Predicted value:\", node.value)\n",
    "    else:\n",
    "        print(indent + \"Feature\", node.feature, \"<=\", node.threshold)\n",
    "        print(indent + \"--> True:\")\n",
    "        print_tree(node.true_branch, depth + 1)\n",
    "        print(indent + \"--> False:\")\n",
    "        print_tree(node.false_branch, depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb7232b0-bbdb-4bfb-b511-d9ea51d0273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris(as_frame=True)\n",
    "X = data[\"data\"].values\n",
    "y = data[\"target\"].values\n",
    "### \n",
    "### Your code here \n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b0b8b0-1e45-427a-807b-139f33982976",
   "metadata": {},
   "source": [
    "### Часть 2. Случайный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e052b110-f73b-4440-801a-09787d8f2808",
   "metadata": {},
   "source": [
    "##### [task] 1. Усреднение классификаторов (2 балла)\n",
    "Реализуйте бэггинг над решающими деревьями (усреднение предсказанных вероятностей всего ансамбля или путем \"голосования\"). В качестве базового алгоритма используйте либо наш класс или DecisionTreeClassifier из пакета scikit-learn. \n",
    "\n",
    "##### [task] 2. Бутстрап обучающей выборки (2 балла)\n",
    "Добавим к нашему усреднению предсказаний бутстрап выборки (генерация случайной выборки того же размера с возвращением). Сгенерируйте с помощью него отдельную обучающую выборку для каждого дерева, обучите их и усредните предсказания, как в предыдущем пункте.\n",
    "\n",
    "##### [task] 3. Выбор случайного подмножества признаков при построении нового дерева. (2 балла)\n",
    "Обучайте каждое дерево на отдельной бутстрап-выборке и случайно выбирайте признаки для обучения. \n",
    "\n",
    "##### [task] 4. Подсчитайте метрики оценки качества классификации (accuracy, precision, recall, ROC AUC, F-мера) для каждого из вариантов дерева. Сделайте отдельную функцию для подсчета метрик (1 балл)\n",
    "\n",
    "- Используемый датасет https://archive.ics.uci.edu/dataset/94/spambase\n",
    "- Все гиперпараметры необходимо вынести аргументы соответсвующей функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a25dbc-8f9d-49aa-93b2-79738df7731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "!pip install ucimlrepo\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "spambase = fetch_ucirepo(id=94) \n",
    "  \n",
    "# data (as numpy arrays) \n",
    "X = spambase.data.features.values\n",
    "y = spambase.data.targets.values\n",
    "  \n",
    "# metadata \n",
    "print(spambase.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(spambase.variables)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b0fc3acb-912a-40ee-9eb0-b944bd9cccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab8f30f-8843-4fea-a88b-601e6933843c",
   "metadata": {},
   "source": [
    "#### Часть 3. \n",
    "\n",
    "- [task] Обучите RandomForestClassifier из sklearn на датасете из прошлой части (2 балла)\n",
    "- [task] Подсчитайте accuracy, precision, recall, ROC AUC, F-мера на отложенной выборке. Получилось лучше или хуже по сравнению с вашим вариантом RandomForest (2 балла)\n",
    "- Попробуйте объяснить результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479d992-2f67-422b-9fe3-a703953e7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
