{
  "metadata" : {
    "config" : {
      "dependencies" : {
        
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "spark.master" : "local[1]"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# hw6<br>\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703354493602,
          "endTs" : 1703354494023
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val sc = spark.sparkContext\r\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703354495921,
          "endTs" : 1703354498456
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import sys.process._\r\n",
        "\r\n",
        "\"tree datasets\".!\r\n",
        "\r\n",
        "val df = spark.read.option(\"header\", \"true\").csv(\"datasets/ppkmSentiment/ppkm_dataset.csv\")\r\n",
        "df.show()\r\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "datasets\n",
            "├── DoctorWho\n",
            "│   ├── all-detailsepisodes.csv\n",
            "│   ├── all-scripts.csv\n",
            "│   ├── dwguide.csv\n",
            "│   └── imdb_details.csv\n",
            "├── HouseMDTranscripts\n",
            "│   ├── season1.csv\n",
            "│   ├── season2.csv\n",
            "│   ├── season3.csv\n",
            "│   ├── season4.csv\n",
            "│   ├── season5.csv\n",
            "│   ├── season6.csv\n",
            "│   ├── season7.csv\n",
            "│   └── season8.csv\n",
            "├── IMDBSentimentAnalysis\n",
            "│   ├── Test.csv\n",
            "│   ├── Train.csv\n",
            "│   └── Valid.csv\n",
            "├── StarWars\n",
            "│   ├── SW_EpisodeIV.txt\n",
            "│   ├── SW_EpisodeV.txt\n",
            "│   └── SW_EpisodeVI.txt\n",
            "├── TopCompaniesTweets\n",
            "│   ├── Company.csv\n",
            "│   ├── Company_Tweet.csv\n",
            "│   └── Tweet.csv\n",
            "└── ppkmSentiment\n",
            "    ├── ppkm_dataset.csv\n",
            "    ├── ppkm_test.csv\n",
            "    └── stopwordv1.txt\n",
            "\n",
            "6 directories, 24 files\n",
            "+-------+--------------------+\n",
            "|  class|             comment|\n",
            "+-------+--------------------+\n",
            "|positif|Kami siap laksana...|\n",
            "|positif|Siap melaksanakan...|\n",
            "|positif|Siap dukung dan s...|\n",
            "|positif|Langkah 3M ini su...|\n",
            "|positif|Siap amankan selu...|\n",
            "|positif|Siap utk di sosia...|\n",
            "|positif|Mendukung kebijak...|\n",
            "|positif|Mari bersama cega...|\n",
            "|positif|Mari kita suksesk...|\n",
            "|positif|Siap kawal dan am...|\n",
            "|positif|Semoga pak gubern...|\n",
            "|positif|Semoga bapak sela...|\n",
            "|positif|     Semangat selalu|\n",
            "|positif|Semoga berhasil.....|\n",
            "|positif|patuhi prokes dan...|\n",
            "|positif|terimakasih atas ...|\n",
            "|positif|Sehat selalu pemi...|\n",
            "|positif|Semoga selalu dib...|\n",
            "|positif|semoga pandemi se...|\n",
            "|positif|Makasih bu, sukse...|\n",
            "+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703413068322,
          "endTs" : 1703413070073
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import org.apache.spark.ml.{Pipeline, PipelineModel}\r\n",
        "import org.apache.spark.ml.classification.LogisticRegression\r\n",
        "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\r\n",
        "import org.apache.spark.ml.feature.{HashingTF, Tokenizer, IDF, StringIndexer}\r\n",
        "import org.apache.spark.sql.types.{StringType, StructField, StructType}\r\n",
        "import org.apache.spark.sql.{SparkSession, Dataset, Encoders}\r\n",
        "\r\n",
        "case class TextLabel(label: String, text: String)\r\n",
        "val sparkk = SparkSession.builder()\r\n",
        "  .appName(\"IMDB Sentiment Analysis\")\r\n",
        "  .getOrCreate()\r\n",
        "\r\n",
        "val schema = new StructType(Array(\r\n",
        "  StructField(\"label\", StringType, true),\r\n",
        "  StructField(\"text\", StringType, true)\r\n",
        "))\r\n",
        "\r\n",
        "import spark.implicits._\r\n",
        "val trainData = sparkk.read.option(\"header\", \"true\").schema(schema).csv(\"datasets/IMDBSentimentAnalysis/Train.csv\").as[TextLabel]\r\n",
        "val validData = sparkk.read.option(\"header\", \"true\").schema(schema).csv(\"datasets/IMDBSentimentAnalysis/Valid.csv\").as[TextLabel]\r\n",
        "\r\n",
        "\r\n",
        "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\r\n",
        "val hashingTF = new HashingTF().setInputCol(tokenizer.getOutputCol).setOutputCol(\"rawFeatures\").setNumFeatures(10000)\r\n",
        "val idf = new IDF().setInputCol(hashingTF.getOutputCol).setOutputCol(\"features\")\r\n",
        "val labelStringIdx = new StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").setHandleInvalid(\"skip\")t\n",
        "\r\n",
        "val lr = new LogisticRegression().setLabelCol(\"indexedLabel\").setFeaturesCol(\"features\").setMaxIter(10).setRegParam(0.1) \r\n",
        "\r\n",
        "val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, idf, labelStringIdx, lr))\r\n",
        "val model = pipeline.fit(trainData)\r\n",
        "val predictions = model.transform(validData)\r\n",
        "\r\n",
        "val evaluator = new MulticlassClassificationEvaluator()\r\n",
        "  .setLabelCol(\"indexedLabel\")\r\n",
        "  .setPredictionCol(\"prediction\")\r\n",
        "  .setMetricName(\"accuracy\")\r\n",
        "\r\n",
        "val accuracy = evaluator.evaluate(predictions)\r\n",
        "println(s\"Accuracy = $accuracy\")\r\n",
        "\r\n",
        "spark.stop()\r\n"
      ],
      "outputs" : [
        {
          "ename" : "org.apache.spark.SparkException",
          "evalue" : "Job aborted due to stage failure: Task 0 in stage 139.0 failed 1 times, most recent failure: Lost task 0.0 in stage 139.0 (TID 139, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$1: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException\n\nDriver stacktrace:",
          "traceback" : [
            "org.apache.spark.scheduler.DAGScheduler,org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages,DAGScheduler.scala,1925",
            "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1,apply,DAGScheduler.scala,1913",
            "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1,apply,DAGScheduler.scala,1912",
            "scala.collection.mutable.ResizableArray$class,foreach,ResizableArray.scala,59",
            "scala.collection.mutable.ArrayBuffer,foreach,ArrayBuffer.scala,48",
            "org.apache.spark.scheduler.DAGScheduler,abortStage,DAGScheduler.scala,1912",
            "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1,apply,DAGScheduler.scala,948",
            "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1,apply,DAGScheduler.scala,948",
            "scala.Option,foreach,Option.scala,257",
            "org.apache.spark.scheduler.DAGScheduler,handleTaskSetFailed,DAGScheduler.scala,948",
            "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop,doOnReceive,DAGScheduler.scala,2146",
            "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop,onReceive,DAGScheduler.scala,2095",
            "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop,onReceive,DAGScheduler.scala,2084",
            "org.apache.spark.util.EventLoop$$anon$1,run,EventLoop.scala,49",
            "org.apache.spark.scheduler.DAGScheduler,runJob,DAGScheduler.scala,759",
            "org.apache.spark.SparkContext,runJob,SparkContext.scala,2067",
            "org.apache.spark.SparkContext,runJob,SparkContext.scala,2164",
            "org.apache.spark.rdd.RDD$$anonfun$fold$1,apply,RDD.scala,1143",
            "org.apache.spark.rdd.RDDOperationScope$,withScope,RDDOperationScope.scala,151",
            "org.apache.spark.rdd.RDDOperationScope$,withScope,RDDOperationScope.scala,112",
            "org.apache.spark.rdd.RDD,withScope,RDD.scala,385",
            "org.apache.spark.rdd.RDD,fold,RDD.scala,1137",
            "org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1,apply,RDD.scala,1206",
            "org.apache.spark.rdd.RDDOperationScope$,withScope,RDDOperationScope.scala,151",
            "org.apache.spark.rdd.RDDOperationScope$,withScope,RDDOperationScope.scala,112",
            "org.apache.spark.rdd.RDD,withScope,RDD.scala,385",
            "org.apache.spark.rdd.RDD,treeAggregate,RDD.scala,1182",
            "org.apache.spark.mllib.feature.IDF,fit,IDF.scala,54",
            "org.apache.spark.ml.feature.IDF,fit,IDF.scala,92",
            "org.apache.spark.ml.feature.IDF,fit,IDF.scala,68",
            "org.apache.spark.ml.Pipeline$$anonfun$fit$2,apply,Pipeline.scala,153",
            "org.apache.spark.ml.Pipeline$$anonfun$fit$2,apply,Pipeline.scala,149",
            "scala.collection.Iterator$class,foreach,Iterator.scala,891",
            "scala.collection.AbstractIterator,foreach,Iterator.scala,1334",
            "scala.collection.IterableViewLike$Transformed$class,foreach,IterableViewLike.scala,44",
            "scala.collection.SeqViewLike$AbstractTransformed,foreach,SeqViewLike.scala,37",
            "org.apache.spark.ml.Pipeline,fit,Pipeline.scala,149",
            "notebook0.Cell3$264,<init>,Cell3,31",
            "sun.reflect.NativeConstructorAccessorImpl,newInstance0,NativeConstructorAccessorImpl.java,-2",
            "sun.reflect.NativeConstructorAccessorImpl,newInstance,NativeConstructorAccessorImpl.java,62",
            "sun.reflect.DelegatingConstructorAccessorImpl,newInstance,DelegatingConstructorAccessorImpl.java,45",
            "java.lang.reflect.Constructor,newInstance,Constructor.java,423",
            "polynote.kernel.interpreter.scal.ScalaInterpreter,polynote$kernel$interpreter$scal$ScalaInterpreter$$createInstance,ScalaInterpreter.scala,173",
            "polynote.kernel.interpreter.scal.ScalaInterpreter$$anonfun$polynote$kernel$interpreter$scal$ScalaInterpreter$$runClass$3$$anonfun$apply$24,apply,ScalaInterpreter.scala,185",
            "zio.blocking.package$Blocking$Service$$anonfun$effectBlockingInterrupt$1$$anonfun$apply$3$$anonfun$apply$4,apply,package.scala,126",
            "zio.blocking.package$Blocking$Service$$anonfun$effectBlockingInterrupt$1$$anonfun$apply$3$$anonfun$apply$4,apply,package.scala,118",
            "zio.ZIO$$anonfun$effectSuspend$1,apply,ZIO.scala,2782",
            "zio.ZIO$$anonfun$effectSuspend$1,apply,ZIO.scala,2782",
            "zio.internal.FiberContext,liftedTree1$1,FiberContext.scala,571",
            "zio.internal.FiberContext,evaluateNow,FiberContext.scala,571",
            "zio.internal.FiberContext,zio$internal$FiberContext$$run$body$1,FiberContext.scala,770",
            "zio.internal.FiberContext$$anonfun$28,run,FiberContext.scala,770",
            "polynote.kernel.interpreter.CellExecutor$$anon$3$$anonfun$run$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$2,apply$mcV$sp,CellExecutor.scala,34",
            "polynote.kernel.interpreter.CellExecutor$$anon$3$$anonfun$run$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$2,apply,CellExecutor.scala,34",
            "polynote.kernel.interpreter.CellExecutor$$anon$3$$anonfun$run$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$2,apply,CellExecutor.scala,34",
            "scala.util.DynamicVariable,withValue,DynamicVariable.scala,58",
            "scala.Console$,withErr,Console.scala,80",
            "polynote.kernel.interpreter.CellExecutor$$anon$3$$anonfun$run$1$$anonfun$apply$mcV$sp$1,apply$mcV$sp,CellExecutor.scala,33",
            "polynote.kernel.interpreter.CellExecutor$$anon$3$$anonfun$run$1$$anonfun$apply$mcV$sp$1,apply,CellExecutor.scala,33",
            "polynote.kernel.interpreter.CellExecutor$$anon$3$$anonfun$run$1$$anonfun$apply$mcV$sp$1,apply,CellExecutor.scala,33",
            "scala.util.DynamicVariable,withValue,DynamicVariable.scala,58",
            "scala.Console$,withOut,Console.scala,53",
            "polynote.kernel.interpreter.CellExecutor$$anon$3$$anonfun$run$1,apply$mcV$sp,CellExecutor.scala,32",
            "polynote.kernel.interpreter.CellExecutor$$anon$3$$anonfun$run$1,apply,CellExecutor.scala,31",
            "polynote.kernel.interpreter.CellExecutor$$anon$3$$anonfun$run$1,apply,CellExecutor.scala,31",
            "polynote.kernel.package$,withContextClassLoader,package.scala,67",
            "polynote.kernel.interpreter.CellExecutor$$anon$3,run,CellExecutor.scala,30",
            "java.util.concurrent.ThreadPoolExecutor,runWorker,ThreadPoolExecutor.java,1149",
            "java.util.concurrent.ThreadPoolExecutor$Worker,run,ThreadPoolExecutor.java,624",
            "java.lang.Thread,run,Thread.java,750"
          ],
          "output_type" : "error"
        }
      ]
    }
  ]
}